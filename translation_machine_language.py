# -*- coding: utf-8 -*-
"""Translation_Machine_Language.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cPx3HaAidO4vKUnhb7YWNPWU4mOe9H0n

#Train Model
"""

# Import all packages required
import pickle
import pathlib
import random
import string
import re
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""## 1.Load data"""



lan_1 = pickle.load(open("/content/DS_5_train_input",'rb'))
lan_2 = pickle.load(open("/content/DS_5_train_output",'rb'))

"""## 2.Visualize data"""

# count unique
from collections import Counter
uniq_train = []
for x in lan_1:
  #print(x)
  uniq_train.extend(x.strip().split(" "))
cnt = Counter(uniq_train)
print(cnt)

uniq_out = []
for x in lan_2:
  #print(x)
  uniq_out.extend(x.strip().split(" "))
cnt1 = Counter(uniq_out)
print(cnt1)
print("number of unique words in input",len(cnt))
print("number of unique words in output",len(cnt1))

"""## 3.Check max length of the strings in the two languages"""

# check max length of the output 
mx_len = 0
mx_ind = 0
for i,op in enumerate(lan_2):
  # print(op)
  if len(op) > mx_len:
    mx_len = len(op)
    mx_ind = i

print(mx_len)
# print(lan_1[mx_ind])
# print(len(lan_1[mx_ind]))
# print(lan_1[mx_ind])
# print(lan_2[mx_ind])
# print(len(lan_2[mx_ind]))

# The longest length of the output string is of 211. This could be set as the maximum size of the vocabulary

"""## Preprocess the data

## 4.Form pairs of the two language strings
"""

# create text pairs from language 1 and language 2
text_pairs = []
for i in range(len(lan_1)):
    l1 = lan_1[i]
    l2 = "[start] " + lan_2[i] + " [end]"
    # l2 = lan_2[i]
    text_pairs.append((l1, l2))

"""### 5.Split the data"""

# splitting the data to train, val and test
import random
random.shuffle(text_pairs)
num_val_samples = int(0.15 * len(text_pairs))
num_train_samples = len(text_pairs) - 2 * num_val_samples
train_pairs = text_pairs[:num_train_samples]
val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]
test_pairs = text_pairs[num_train_samples + num_val_samples :]

print(f"{len(text_pairs)} total pairs")
print(f"{len(train_pairs)} training pairs")
print(f"{len(val_pairs)} validation pairs")
print(f"{len(test_pairs)} test pairs")

"""### Vectorization"""

# defining the vocab sizes and seq len
vocab_size = 35
sequence_length = mx_len
batch_size = 64

from tensorflow.keras.layers import TextVectorization

lan_1_vectorization = TextVectorization(
    max_tokens=vocab_size, output_mode="int", output_sequence_length=sequence_length,
)
lan_2_vectorization = TextVectorization(
    max_tokens=vocab_size,
    output_mode="int",
    output_sequence_length=sequence_length + 1,
    # standardize=custom_standardization,
)
train_lan_1_texts = [pair[0] for pair in train_pairs]
train_lan_2_texts = [pair[1] for pair in train_pairs]
lan_1_vectorization.adapt(train_lan_1_texts)
lan_2_vectorization.adapt(train_lan_2_texts)

"""### save vecotrization

### Dataset creation
"""

def format_dataset(l1, l2):
    l1 = lan_1_vectorization(l1)
    l2 = lan_2_vectorization(l2)
    return ({"encoder_inputs": l1, "decoder_inputs": l2[:, :-1],}, l2[:, 1:])


def make_dataset(pairs):
    l1_texts, l2_texts = zip(*pairs)
    l1_texts = list(l1_texts)
    l2_texts = list(l2_texts)
    dataset = tf.data.Dataset.from_tensor_slices((l1_texts, l2_texts))
    dataset = dataset.batch(batch_size)
    dataset = dataset.map(format_dataset)
    return dataset.shuffle(2048).prefetch(16).cache()


train_ds = make_dataset(train_pairs)
val_ds = make_dataset(val_pairs)

for inputs, targets in train_ds.take(1):
    print(f'inputs["encoder_inputs"].shape: {inputs["encoder_inputs"].shape}')
    print(f'inputs["decoder_inputs"].shape: {inputs["decoder_inputs"].shape}')
    print(f"targets.shape: {targets.shape}")

"""## Define and Train model

### Define layers
"""

# define model

class TransformerEncoder(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super(TransformerEncoder, self).__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.dense_proj = keras.Sequential(
            [layers.Dense(dense_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.supports_masking = True

    def call(self, inputs, mask=None):
        if mask is not None:
            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype="int32")
        attention_output = self.attention(
            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask
        )
        proj_input = self.layernorm_1(inputs + attention_output)
        proj_output = self.dense_proj(proj_input)
        return self.layernorm_2(proj_input + proj_output)
        
    def get_config(self):
        config = super().get_config()
        config.update({
            "num_heads": self.num_heads,
            "dense_dim": self.dense_dim,
            "embed_dim": self.embed_dim
        })
        return config


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super(PositionalEmbedding, self).__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)
    
    def get_config(self):
        config = super().get_config()
        config.update({
            "sequence_length": self.sequence_length,
            "vocab_size": self.vocab_size,
            "embed_dim": self.embed_dim
        })
        return config


class TransformerDecoder(layers.Layer):
    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):
        super(TransformerDecoder, self).__init__(**kwargs)
        self.embed_dim = embed_dim
        self.latent_dim = latent_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.dense_proj = keras.Sequential(
            [layers.Dense(latent_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, mask=None):
        causal_mask = self.get_causal_attention_mask(inputs)
        if mask is not None:
            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype="int32")
            padding_mask = tf.minimum(padding_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        proj_output = self.dense_proj(out_2)
        return self.layernorm_3(out_2 + proj_output)

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)

    def get_config(self):
      config = super().get_config()
      config.update({
          "num_heads": self.num_heads,
          "latent_dim": self.latent_dim,
          "embed_dim": self.embed_dim
      })
      return config

"""### Build model"""

embed_dim = 256
latent_dim = 512
num_heads = 8

encoder_inputs = keras.Input(shape=(None,), dtype="int64", name="encoder_inputs")
x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)
encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)
encoder = keras.Model(encoder_inputs, encoder_outputs)

decoder_inputs = keras.Input(shape=(None,), dtype="int64", name="decoder_inputs")
encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name="decoder_state_inputs")
x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)
x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)
x = layers.Dropout(0.5)(x)
decoder_outputs = layers.Dense(vocab_size, activation="softmax")(x)
decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)

decoder_outputs = decoder([decoder_inputs, encoder_outputs])
transformer = keras.Model(
    [encoder_inputs, decoder_inputs], decoder_outputs, name="transformer"
)

"""### Model Fit"""

epochs = 400  # This should be at least 30 for convergence
from keras.callbacks import EarlyStopping, ModelCheckpoint

# define callbacks
earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')
mcp_save = ModelCheckpoint('/content/sample_data/transformer_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')

transformer.summary()
transformer.compile(
    "rmsprop", loss="sparse_categorical_crossentropy", metrics=["accuracy"]
)
transformer.fit(train_ds, epochs=epochs, validation_data=val_ds,callbacks=[earlyStopping,mcp_save] )

"""### Model save"""

transformer.save("transformer.h5")
transformer.save_weights("weights.h5")

conf = transformer.to_json()
import json
with open("config.json",'w') as fh:
  json.dump(conf,fh)

with open("config.json",'r') as fh:
  # new_model = tf.keras.models.model_from_json(json.load(fh))
  config = json.load(fh)

"""## Model Test"""

# config = transformer.get_config()
custom_objects = {"TransformerEncoder": TransformerEncoder,
                  "TransformerDecoder":TransformerDecoder,
                  "PositionalEmbedding":PositionalEmbedding }
with keras.utils.custom_object_scope(custom_objects):
    # new_model = keras.Model.from_config(config)
    new_model = tf.keras.models.model_from_json(config)

new_model.load_weights("weights.h5")
# transformer = keras.models.load_model("/content/transformer.h5")
l2_vocab = lan_2_vectorization.get_vocabulary()
l2_index_lookup = dict(zip(range(len(l2_vocab)), l2_vocab))
max_decoded_sentence_length = mx_len



def decode_sequence(input_sentence):
    tokenized_input_sentence = lan_1_vectorization([input_sentence])
    decoded_sentence = "[start]"
    for i in range(max_decoded_sentence_length):
        tokenized_target_sentence = lan_2_vectorization([decoded_sentence])[:, :-1]
        # print(tokenized_input_sentence)
        # print(tokenized_target_sentence)
        predictions = new_model([tokenized_input_sentence, tokenized_target_sentence])
        # print(predictions)

        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = l2_index_lookup[sampled_token_index]
        decoded_sentence += " " + sampled_token

        if sampled_token == "[end]":
            break
    return decoded_sentence


test_l1_texts = [pair[0] for pair in test_pairs]
test_l2_texts = [pair[1] for pair in test_pairs]
# for _ in range(30):
#     input_sentence = random.choice(test_l1_texts)
#     translated = decode_sequence(input_sentence)
#     print()
translated = decode_sequence(test_l1_texts[8])
print("out",translated)
print("inp",test_l2_texts[8])

"""# Test 
Use this part of the code to test

First - define the layers
"""

import pickle
import pathlib
import random
import string
import re
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class TransformerEncoder(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super(TransformerEncoder, self).__init__(**kwargs)
        self.embed_dim = embed_dim
        self.dense_dim = dense_dim
        self.num_heads = num_heads
        self.attention = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.dense_proj = keras.Sequential(
            [layers.Dense(dense_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.supports_masking = True

    def call(self, inputs, mask=None):
        if mask is not None:
            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype="int32")
        attention_output = self.attention(
            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask
        )
        proj_input = self.layernorm_1(inputs + attention_output)
        proj_output = self.dense_proj(proj_input)
        return self.layernorm_2(proj_input + proj_output)
        
    def get_config(self):
        config = super().get_config()
        config.update({
            "num_heads": self.num_heads,
            "dense_dim": self.dense_dim,
            "embed_dim": self.embed_dim
        })
        return config


class PositionalEmbedding(layers.Layer):
    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):
        super(PositionalEmbedding, self).__init__(**kwargs)
        self.token_embeddings = layers.Embedding(
            input_dim=vocab_size, output_dim=embed_dim
        )
        self.position_embeddings = layers.Embedding(
            input_dim=sequence_length, output_dim=embed_dim
        )
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim

    def call(self, inputs):
        length = tf.shape(inputs)[-1]
        positions = tf.range(start=0, limit=length, delta=1)
        embedded_tokens = self.token_embeddings(inputs)
        embedded_positions = self.position_embeddings(positions)
        return embedded_tokens + embedded_positions

    def compute_mask(self, inputs, mask=None):
        return tf.math.not_equal(inputs, 0)
    
    def get_config(self):
        config = super().get_config()
        config.update({
            "sequence_length": self.sequence_length,
            "vocab_size": self.vocab_size,
            "embed_dim": self.embed_dim
        })
        return config


class TransformerDecoder(layers.Layer):
    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):
        super(TransformerDecoder, self).__init__(**kwargs)
        self.embed_dim = embed_dim
        self.latent_dim = latent_dim
        self.num_heads = num_heads
        self.attention_1 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.attention_2 = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim
        )
        self.dense_proj = keras.Sequential(
            [layers.Dense(latent_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm_1 = layers.LayerNormalization()
        self.layernorm_2 = layers.LayerNormalization()
        self.layernorm_3 = layers.LayerNormalization()
        self.supports_masking = True

    def call(self, inputs, encoder_outputs, mask=None):
        causal_mask = self.get_causal_attention_mask(inputs)
        if mask is not None:
            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype="int32")
            padding_mask = tf.minimum(padding_mask, causal_mask)

        attention_output_1 = self.attention_1(
            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask
        )
        out_1 = self.layernorm_1(inputs + attention_output_1)

        attention_output_2 = self.attention_2(
            query=out_1,
            value=encoder_outputs,
            key=encoder_outputs,
            attention_mask=padding_mask,
        )
        out_2 = self.layernorm_2(out_1 + attention_output_2)

        proj_output = self.dense_proj(out_2)
        return self.layernorm_3(out_2 + proj_output)

    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i >= j, dtype="int32")
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0,
        )
        return tf.tile(mask, mult)

    def get_config(self):
      config = super().get_config()
      config.update({
          "num_heads": self.num_heads,
          "latent_dim": self.latent_dim,
          "embed_dim": self.embed_dim
      })
      return config

"""## Second
we will be using the train data to generate the vectorization and thereby vocabulary.
Following files are required to be loaded:
1. train_input
2. train_output
3. test_input
4. test_output
5. trained model

follow the path descriptions in the code below to give relevant paths.
"""

def load_model(model_path):
  custom_objects = {"TransformerEncoder": TransformerEncoder,
                    "TransformerDecoder":TransformerDecoder,
                    "PositionalEmbedding":PositionalEmbedding }
  with keras.utils.custom_object_scope(custom_objects):
      new_model = keras.models.load_model(model_path)
  return new_model

lan_1 = pickle.load(open("enter_the_path_to_train_input_language",'rb'))
lan_2 = pickle.load(open("enter_the_path_to_train_output_language",'rb'))

lan_test_1 = pickle.load(open("test_input_language_path",'rb'))
lan_test_2 = pickle.load(open("test_input_language_path",'rb'))

new_model = load_model("path_to_model")

# SAMPLE CODE
# lan_1 = pickle.load(open("/content/DS_5_train_input",'rb'))
# lan_2 = pickle.load(open("/content/DS_5_train_output",'rb'))
# lan_test_1 = [pair[0] for pair in test_pairs]
# lan_test_2 = [pair[1] for pair in test_pairs]

# new_model = load_model("/content/transformer.h5")

def decode_sequence(new_model,l2_index_lookup,input_sentence,max_decoded_sentence_length,lan_1_vectorization,lan_2_vectorization):
    tokenized_input_sentence = lan_1_vectorization([input_sentence])
    decoded_sentence = "[start]"
    for i in range(max_decoded_sentence_length):
        tokenized_target_sentence = lan_2_vectorization([decoded_sentence])[:, :-1]
        predictions = new_model([tokenized_input_sentence, tokenized_target_sentence])

        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = l2_index_lookup[sampled_token_index]
        decoded_sentence += " " + sampled_token

        if sampled_token == "end":
            break
    decoded_sentence = decoded_sentence.replace("[start] ","")
    decoded_sentence = decoded_sentence.replace("end","")


    return decoded_sentence


mx_len = 211

text_pairs = []
for i in range(len(lan_test_1)):
  l1 = lan_test_1[i]
  l2 = "[start] " + lan_test_2[i] + " [end]"
  text_pairs.append((l1, l2))


train_pairs = []
for i in range(len(lan_1)):
    l1 = lan_1[i]
    l2 = "[start] " + lan_2[i] + " [end]"
    # l2 = lan_2[i]
    train_pairs.append((l1, l2))
  

train_lan_1_texts = [pair[0] for pair in train_pairs]
train_lan_2_texts = [pair[1] for pair in train_pairs]
lan_1_vectorization.adapt(train_lan_1_texts)
lan_2_vectorization.adapt(train_lan_2_texts)


l2_vocab = lan_2_vectorization.get_vocabulary()
l2_index_lookup = dict(zip(range(len(l2_vocab)), l2_vocab))
max_decoded_sentence_length = mx_len

test_l1_texts = [pair[0] for pair in text_pairs]
test_l2_texts = [pair[1] for pair in text_pairs]
  
predictions = []
for i in range(len(test_l1_texts)):
  predicted = decode_sequence(new_model,l2_index_lookup,test_l1_texts[i],max_decoded_sentence_length,lan_1_vectorization,lan_2_vectorization)
  predictions.append(predicted)

  # print("pred:",predicted)
  # print("original:",test_l2_texts[i])
  # print("____________________________________________________________________")


correct_count = 0
total_count = 0
for i,pred_string in enumerate(predictions):
  pred_string = pred_string.split()
  ground_truth = test_l2_texts[i].split()
  ground_truth = ground_truth[1:len(ground_truth)-1]
  # print(ground_truth)
  leng = min(len(pred_string),len(ground_truth))
  for j in range(leng):
    if ground_truth[j] == pred_string[j]:
      correct_count += 1
    total_count += 1
accuracy = correct_count/total_count

print(accuracy)