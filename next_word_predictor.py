# -*- coding: utf-8 -*-
"""Next_Word_Predictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F_o--2YxqiVKPAzPLhuGtcSA64nMwnBQ

## Import all the libraries
"""

import pickle
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
import numpy as np
import os

"""## Upload files"""

from google.colab import files
print(" upload input file")
input_string = files.upload()

print("upload the output file")
output_word = files.upload()

print("upload the whole data")
whole_data = files.upload()

"""## Load data"""

output_word = pickle.load(open("../input/dlhw33/DS_5_train_input_nextWord",'rb'))
input_string = pickle.load(open("../input/dlhw33/DS_5_train_input_prefixList",'rb'))
whole_data = pickle.load(open("../input/dlhw33/DS_5_train_input (1)",'rb'))

"""## Visualize Data"""

for _ in range(10):
  print(f'{input_string[_]}, out {output_word[_]}')

print("minimum length of the string in the input is ",len(min(input_string, key=len)))
print("maximum length of the string in the input is ",len(max(input_string, key=len)))

"""### The unique values in the input_string"""

result = {x for l in input_string for x in l}
print(result)
print(len(result))

unique = set()
for _ in input_string:
  unique.update(set(_))
print(unique)
print(len(unique))

"""### A look at the whole data"""

for _ in range(10):
  print(whole_data[_])

"""### Unique values in the whole data"""

unique = set()
for _ in whole_data:
  unique.update(set(_))
print(unique)
print(len(unique))

print("minimum length of the string in the input is ",len(min(whole_data, key=len)))
print("maximum length of the string in the input is ",len(max(whole_data, key=len)))

"""## Create new data set"""

def create_n_gram_data(input_data):
    new_data_x = []
    new_data_y = []
    for line in input_data:
        for i in range(1,len(line)):
            new_data_x.append(line[:i])
            new_data_y.append(line[i])
        
    return new_data_x, new_data_y

new_input_string, new_output_string = create_n_gram_data(input_string)
print(len(new_input_string))

"""## Tokenize"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(whole_data)
pickle.dump(tokenizer,open('tokenizer.pkl','wb'))

"""## Tokenize input strings """

seq_input = tokenizer.texts_to_sequences(new_input_string)
# print(se)
# print(input_string)
# print(len(se[0]))
# print(len(input_string[0]))

for _ in range(5):
  print(f"string is {new_input_string[_]} \n and the tokenized string is {seq_input[_]}")

vocab_size = len(tokenizer.word_index) + 1
input_sequence_length = 50
print(vocab_size)

"""## Making the input into fixed length with padding or trimming"""

seq_input_trim = []

for _ in range(len(seq_input)):
  if len(seq_input[_]) > input_sequence_length:
    words = seq_input[_][-input_sequence_length:]
  else:
    padding = [0 for _ in range(input_sequence_length-len(seq_input[_]))]
    words = padding + seq_input[_]
  seq_input_trim.append(words)
seq_input_trim = np.array(seq_input_trim)
print(seq_input_trim[:10])
print(type(seq_input_trim))
print("length of the sequences : ",len(seq_input_trim))

"""## Get the tokenized output"""

seq_out_word = tokenizer.texts_to_sequences(new_output_string)
seq_out_word[:10]
seq_out_word = np.array(seq_out_word)
print(type(seq_out_word))
print(seq_out_word[:10])

"""## Change output to categorical output"""

seq_cat_out = to_categorical(seq_out_word, num_classes=vocab_size)
seq_cat_out[:10]

"""## Split into train and test """

split_at = int(0.8*len(seq_input_trim))
train_data_x = seq_input_trim[:split_at]
train_data_y = seq_cat_out[:split_at]
test_data_x = seq_input_trim[split_at:]
test_data_y = seq_cat_out[split_at:]

"""## Build a model

"""

model = Sequential()
model.add(Embedding(vocab_size, 150, input_length=input_sequence_length))
model.add(LSTM(256, return_sequences=True))
model.add(LSTM(256))
model.add(Dense(256, activation ="relu"))
model.add(Dense(vocab_size, activation="softmax"))

model.summary()

from keras.callbacks import EarlyStopping, ModelCheckpoint

# define callbacks
earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')
mcp_save = ModelCheckpoint('LSTM_model.h5',verbose=0, save_best_only=True, monitor='val_loss', mode='min')

model.compile(
   loss="categorical_crossentropy",optimizer = Adam(learning_rate=0.001), metrics=["accuracy"]
)

"""### Fit Model

"""

epochs = 50
batch_size = 128
model.fit(seq_input_trim,seq_cat_out, epochs=epochs,batch_size=batch_size, validation_split=0.2,callbacks=[earlyStopping,mcp_save] )

from tensorflow import keras
new_model = keras.models.load_model("./LSTM_model.h5")

new_model.evaluate(test_data_x,test_data_y)

"""## Test
The testing code is not explicitly written because of the recent changes in the expectation of the projects and the test results will be shared as a part of HW8
"""

def predict_next_words(model,tokenizer,sequence):
  # sequence = tokenizer.texts_to_sequences(text)
  # sequence = np.array(sequence)
  
  preds = np.argmax(model.predict(sequence))
  predicted_word = ""
  for key, value in tokenizer.word_index.items():
    if value == preds:
      predicted_word = key
      break
  print(predicted_word)
  return predicted_word

# predict_next_words(model, tokenizer, test_data_x[0])
# print(test_data_x[0].reshape(1,10).shape)
# print(model.input_shape)
# x= test_data_x[0].reshape(1,10)
correct = 0
for _ in range(len(test_data_x)):
  x = test_data_x[_].reshape(1,50)
  y = model.predict(x)
  a = np.argmax(y)
  b = np.argmax(test_data_y[_])
  # print(np.argmax(y))
  # print(np.argmax(test_data_y[_]))
  if a==b:
    correct += 1

print(correct/len(test_data_x))